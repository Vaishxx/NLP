{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16554e3d",
   "metadata": {},
   "source": [
    "##### LOWERCASING || REMOVE HTML TAGS || REMOVE URLS || REMOVE PUNCTUATION || CHAT WORD TREATMENT || SPELLING COREECTION || REMOVING STOP WORDS|| HANDLING EMOJIS || TOKENIZATION || STEMMING || LEMMATIZATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "991e0cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b669c8f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv.zip')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76299eff",
   "metadata": {},
   "source": [
    "# Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8781b6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c93a916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review']=df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816614c",
   "metadata": {},
   "source": [
    "### Remove HTML Tags or Unwanted test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eee8e0",
   "metadata": {},
   "source": [
    "#### Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bca8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91caed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc4d5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. the filming tec...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. the filming tec...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76edca9b",
   "metadata": {},
   "source": [
    "# Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b19ae0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern = re.compile(r'http?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9441a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kjfb kjn sjnkd knsld  rfnwo wefkw '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "text='kjfb kjn sjnkd knsld http://www.google.com rfnwo wefkw www.wikepedia.com'\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2ac8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f3291",
   "metadata": {},
   "source": [
    "# Removing Punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8053e907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76ccc334",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude= string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f226c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,'')\n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "657663ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'string With  Punctuation'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='string With . Punctuation[]//'\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b324002",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5756bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With  Punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(text))\n",
    "time01=time.time() - start\n",
    "print(time01)\n",
    "#this shows that this is a time consuming process for when working on large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96bccb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#therefore here's the alternative tech which is more prefered\n",
    "def remove_punc1(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05c21653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string With  Punctuation\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "print(remove_punc1(text))\n",
    "time02=time.time() - start\n",
    "print(time02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d1a0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time01/time02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['review']=df['review'].apply(remove_punc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f9ea9",
   "metadata": {},
   "source": [
    "# Chat Word treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46c007a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words={\n",
    "    'brb':'be right back',\n",
    "    'ttyl': 'talk to you later',\n",
    "    'wyd': 'what you doin',\n",
    "    'hry':'how are you',\n",
    "    'hby':'how about you',\n",
    "    'wbu':'what about you',\n",
    "    'omw':'on my way',\n",
    "    'wya':'where you at',\n",
    "    'lol':'laugh all loud',\n",
    "    'fr':'for real',\n",
    "    'ngl':'not gonna lie',\n",
    "    'np':'np cap',\n",
    "    'gn':'good night',\n",
    "    'gm':'good morining',\n",
    "    'lmao':'laugh my ass out',\n",
    "}\n",
    "#temp word dict,but there are many dict online available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919734bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversation(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.lower() in chat_words:\n",
    "            new_text.append(chat_words[w.lower()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f473c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am excited to see more opportunities that are on my way for real'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_conversation('I am excited to see more opportunities that are omw fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0565b367",
   "metadata": {},
   "source": [
    "# Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4c65f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ca1c613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'she is a good student i guess'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_text='she is a goood studantt i guess'\n",
    "textBlb=TextBlob(incorrect_text)\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e9eb2",
   "metadata": {},
   "source": [
    "# Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b16611eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8135cc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d600be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "505a4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]        \n",
    "    new_text.clear()\n",
    "    return ' '.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5311372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sijcn  woin pr kdn yout   cant    '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords('sijcn and woin pr kdn yout your why cant is her he him')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44769f37",
   "metadata": {},
   "source": [
    "# Handling Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "305b70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove emoji\n",
    "import re \n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U000024C2-\\U0001F251\"  # enclosed characters\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34c7bdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ! How are you ?\n"
     ]
    }
   ],
   "source": [
    "text_with_emojis = \"Hello 😊! How are you 🚀?\"\n",
    "clean_text = remove_emoji(text_with_emojis)\n",
    "print(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31a19eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffae0f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c0204bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is :smiling_face_with_heart-eyes: \n"
     ]
    }
   ],
   "source": [
    "#replace emoji\n",
    "import emoji\n",
    "print(emoji.demojize('Python is 😍 '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c797b74d",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2620ea",
   "metadata": {},
   "source": [
    "### 1. Using Split Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26681e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'a', 'girl']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word tonkenization\n",
    "sent1='I am a girl'\n",
    "sent1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb8221b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'going',\n",
       " 'to',\n",
       " 'delhi',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'stay',\n",
       " 'there',\n",
       " 'for',\n",
       " '3',\n",
       " 'days',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'gonna',\n",
       " 'have',\n",
       " 'fun',\n",
       " 'indeed']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentance tokenization\n",
    "sent2='I am going to delhi . I will stay there for 3 days . I am gonna have fun indeed'\n",
    "sent2.split() #splits on the basis of space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e0ec7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am going to delhi',\n",
       " 'I will stay there for 3 days',\n",
       " 'I am gonna have fun indeed']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c050f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hey!', 'how', 'are', 'you?']\n",
      "['hey', '!', 'how', 'are', 'you', '?']\n",
      "['You Going? I am coming too', 'Wait']\n"
     ]
    }
   ],
   "source": [
    "#problem with split function\n",
    "sent3='hey! how are you?'\n",
    "sent4='hey ! how are you ?'\n",
    "sent5='You Going? I am coming too.Wait' #can focus on pnly one punctuation\n",
    "print(sent3.split())\n",
    "print(sent4.split())\n",
    "print(sent5.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c25d92",
   "metadata": {},
   "source": [
    "### 2. Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db7c654c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "sent6='I am going to delhi!'\n",
    "tokens=re.findall(\"[\\w']+\",sent6)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "33cfb8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roing ',\n",
       " ' erig , wsofn',\n",
       " 'wefn',\n",
       " ' wefnjk',\n",
       " ' wrfn',\n",
       " ' orifno',\n",
       " 'jef',\n",
       " ' ijdf',\n",
       " '']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent7='roing ! erig , wsofn?wefn. wefnjk? wrfn! orifno!jef? ijdf?'\n",
    "sentences=re.compile('[.!?]').split(sent7)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b3c326",
   "metadata": {},
   "source": [
    "### 3. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0496fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "704e942c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'delhi', '!']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "sent01='I am going to delhi!'\n",
    "word_tokenize(sent01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "81a10c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sun was setting, casting a golden hue over the city.',\n",
       " 'Birds chirped melodiously, while children played in the park, laughing and shouting with joy.',\n",
       " 'Suddenly, a gentle breeze rustled the leaves, creating a soothing symphony.',\n",
       " '“What a perfect evening,” thought Sarah, as she sipped her coffee, savoring the moment.']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='The sun was setting, casting a golden hue over the city. Birds chirped melodiously, while children played in the park, laughing and shouting with joy. Suddenly, a gentle breeze rustled the leaves, creating a soothing symphony. “What a perfect evening,” thought Sarah, as she sipped her coffee, savoring the moment.'\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0057e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent01='I am perusing M.Tech With A.I and D.S'\n",
    "sent02='I am looking forward to see opportunities!'\n",
    "sent03='heres my mail id- vaishxx07@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9e175b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'perusing', 'M.Tech', 'With', 'A.I', 'and', 'D.S']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b3523c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'looking', 'forward', 'to', 'see', 'opportunities', '!']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5083acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heres', 'my', 'mail', 'id-', 'vaishxx07', '@', 'gmail.com']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(sent03)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cd7138",
   "metadata": {},
   "source": [
    "### 4. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c391e959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacyNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading spacy-3.7.6-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "                                              0.0/12.1 MB ? eta -:--:--\n",
      "                                              0.1/12.1 MB 3.6 MB/s eta 0:00:04\n",
      "                                              0.3/12.1 MB 3.4 MB/s eta 0:00:04\n",
      "     -                                        0.5/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "     --                                       0.7/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "     --                                       0.8/12.1 MB 4.0 MB/s eta 0:00:03\n",
      "     ---                                      1.0/12.1 MB 4.1 MB/s eta 0:00:03\n",
      "     ---                                      1.2/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "     ----                                     1.4/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "     -----                                    1.6/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "     -----                                    1.8/12.1 MB 4.4 MB/s eta 0:00:03\n",
      "     ------                                   2.0/12.1 MB 4.6 MB/s eta 0:00:03\n",
      "     -------                                  2.2/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "     -------                                  2.4/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "     --------                                 2.5/12.1 MB 4.5 MB/s eta 0:00:03\n",
      "     --------                                 2.7/12.1 MB 4.4 MB/s eta 0:00:03\n",
      "     ---------                                2.8/12.1 MB 4.3 MB/s eta 0:00:03\n",
      "     ----------                               3.0/12.1 MB 4.4 MB/s eta 0:00:03\n",
      "     ----------                               3.2/12.1 MB 4.4 MB/s eta 0:00:03\n",
      "     ----------                               3.3/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "     -----------                              3.5/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "     ------------                             3.6/12.1 MB 4.2 MB/s eta 0:00:03\n",
      "     ------------                             3.8/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------                            4.0/12.1 MB 4.3 MB/s eta 0:00:02\n",
      "     -------------                            4.2/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------                           4.4/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------                           4.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------                           4.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------                           4.5/12.1 MB 4.2 MB/s eta 0:00:02\n",
      "     ---------------                          4.7/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "     -----------------                        5.2/12.1 MB 4.1 MB/s eta 0:00:02\n",
      "     ------------------                       5.7/12.1 MB 4.0 MB/s eta 0:00:02\n",
      "     --------------------                     6.2/12.1 MB 4.0 MB/s eta 0:00:02\n",
      "     --------------------                     6.3/12.1 MB 3.9 MB/s eta 0:00:02\n",
      "     ---------------------                    6.4/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------------                    6.6/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------------                   6.8/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     ----------------------                   7.0/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     -----------------------                  7.1/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     -----------------------                  7.2/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     ------------------------                 7.5/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------------                7.7/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     -------------------------                7.8/12.1 MB 3.8 MB/s eta 0:00:02\n",
      "     --------------------------               7.9/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "     --------------------------               8.2/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ---------------------------              8.3/12.1 MB 3.7 MB/s eta 0:00:02\n",
      "     ----------------------------             8.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------             8.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     -----------------------------            8.8/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     -----------------------------            9.0/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------           9.2/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     ------------------------------           9.4/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------          9.5/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------          9.6/12.1 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------          9.6/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------------------         9.8/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        10.1/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        10.2/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------------       10.4/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------------       10.5/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     -----------------------------------      10.7/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     -----------------------------------      10.8/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     10.9/12.1 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     11.1/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     -------------------------------------    11.4/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   11.6/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   11.7/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  11.9/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.0/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.1/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.1/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.1/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.1/12.1 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.1/12.1 MB 3.4 MB/s eta 0:00:00\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "                                              0.0/122.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 122.3/122.3 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ----                                     0.2/1.5 MB 4.6 MB/s eta 0:00:01\n",
      "     -------                                  0.3/1.5 MB 3.3 MB/s eta 0:00:01\n",
      "     -----------                              0.4/1.5 MB 3.4 MB/s eta 0:00:01\n",
      "     ------------------                       0.7/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "     ----------------------                   0.8/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "     --------------------------               1.0/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "     -------------------------------          1.2/1.5 MB 3.9 MB/s eta 0:00:01\n",
      "     ----------------------------------       1.3/1.5 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 3.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "                                              0.0/479.7 kB ? eta -:--:--\n",
      "     ---------------                       194.6/479.7 kB 11.5 MB/s eta 0:00:01\n",
      "     -------------------------              327.7/479.7 kB 5.0 MB/s eta 0:00:01\n",
      "     -------------------------------------  471.0/479.7 kB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 479.7/479.7 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "                                              0.0/50.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "                                              0.0/47.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 47.3/47.3 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in f:\\okok\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in f:\\okok\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.9.1-py3-none-any.whl (434 kB)\n",
      "                                              0.0/434.4 kB ? eta -:--:--\n",
      "     ---------------------                  245.8/434.4 kB 7.4 MB/s eta 0:00:01\n",
      "     ---------------------------------      378.9/434.4 kB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 434.4/434.4 kB 4.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in f:\\okok\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in f:\\okok\\lib\\site-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in f:\\okok\\lib\\site-packages (from spacy) (23.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "                                              0.0/182.0 kB ? eta -:--:--\n",
      "     --------------------------------       153.6/182.0 kB 4.6 MB/s eta 0:00:01\n",
      "     -------------------------------------- 182.0/182.0 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.19.0 in f:\\okok\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "                                              0.0/5.4 MB ? eta -:--:--\n",
      "     -                                        0.2/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "     --                                       0.4/5.4 MB 4.5 MB/s eta 0:00:02\n",
      "     ----                                     0.6/5.4 MB 4.5 MB/s eta 0:00:02\n",
      "     -----                                    0.8/5.4 MB 4.5 MB/s eta 0:00:02\n",
      "     -------                                  1.0/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "     --------                                 1.1/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------                                1.3/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------                              1.5/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "     ------------                             1.7/5.4 MB 4.6 MB/s eta 0:00:01\n",
      "     --------------                           1.9/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------                          2.0/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------                         2.3/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------------                        2.4/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------                      2.6/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------                     2.8/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------                   3.0/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------------------                  3.2/5.4 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------                 3.3/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------                 3.3/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "     ------------------------                 3.3/5.4 MB 4.4 MB/s eta 0:00:01\n",
      "     --------------------------               3.5/5.4 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------              3.7/5.4 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------          4.2/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------------------         4.4/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------------------        4.5/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------------------       4.7/5.4 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------------------      4.8/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------------------    5.0/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------------------    5.1/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------------------   5.2/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.3/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.4/5.4 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.4/5.4 MB 3.9 MB/s eta 0:00:00\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.23.3 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.23.3-cp311-none-win_amd64.whl (1.9 MB)\n",
      "                                              0.0/1.9 MB ? eta -:--:--\n",
      "     ----                                     0.2/1.9 MB 3.9 MB/s eta 0:00:01\n",
      "     ------                                   0.3/1.9 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------                             0.6/1.9 MB 4.8 MB/s eta 0:00:01\n",
      "     --------------                           0.7/1.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -------------------                      0.9/1.9 MB 4.4 MB/s eta 0:00:01\n",
      "     -----------------------                  1.1/1.9 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------               1.3/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "     --------------------------------         1.6/1.9 MB 4.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.7/1.9 MB 4.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   1.8/1.9 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.9/1.9 MB 4.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.9/1.9 MB 4.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in f:\\okok\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\okok\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\okok\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in f:\\okok\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\okok\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "                                              0.0/6.6 MB ? eta -:--:--\n",
      "     -                                        0.2/6.6 MB 5.8 MB/s eta 0:00:02\n",
      "     --                                       0.4/6.6 MB 4.9 MB/s eta 0:00:02\n",
      "     ---                                      0.6/6.6 MB 5.0 MB/s eta 0:00:02\n",
      "     -----                                    0.8/6.6 MB 5.2 MB/s eta 0:00:02\n",
      "     ------                                   1.1/6.6 MB 5.3 MB/s eta 0:00:02\n",
      "     -------                                  1.2/6.6 MB 5.2 MB/s eta 0:00:02\n",
      "     --------                                 1.5/6.6 MB 5.2 MB/s eta 0:00:01\n",
      "     ----------                               1.7/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "     -----------                              1.8/6.6 MB 5.1 MB/s eta 0:00:01\n",
      "     -----------                              2.0/6.6 MB 5.0 MB/s eta 0:00:01\n",
      "     ------------                             2.1/6.6 MB 4.8 MB/s eta 0:00:01\n",
      "     -------------                            2.3/6.6 MB 4.7 MB/s eta 0:00:01\n",
      "     --------------                           2.4/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "     ---------------                          2.6/6.6 MB 4.6 MB/s eta 0:00:01\n",
      "     ----------------                         2.7/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "     -----------------                        2.9/6.6 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------                       3.0/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     -------------------                      3.2/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     --------------------                     3.4/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     ---------------------                    3.5/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     ----------------------                   3.6/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------                  3.8/6.6 MB 4.3 MB/s eta 0:00:01\n",
      "     -----------------------                  3.9/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "     ------------------------                 4.1/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "     -------------------------                4.2/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "     --------------------------               4.4/6.6 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------              4.5/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------------------------             4.7/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     -----------------------------            4.8/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------           5.0/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------          5.2/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     --------------------------------         5.4/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------        5.5/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ----------------------------------       5.7/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      5.8/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------------     6.0/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------    6.2/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     --------------------------------------   6.3/6.6 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.5/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.6/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.6/6.6 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.6/6.6 MB 3.8 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in f:\\okok\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in f:\\okok\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.0.4)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in f:\\okok\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "                                              0.0/49.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 49.4/49.4 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in f:\\okok\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\okok\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "                                              0.0/152.6 kB ? eta -:--:--\n",
      "     -----------------------------------    143.4/152.6 kB 2.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 152.6/152.6 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\okok\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in f:\\okok\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\okok\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 pydantic-2.9.1 pydantic-core-2.23.3 shellingham-1.5.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.5 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1a2a8293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a20deb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79b87acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(sent01)\n",
    "doc2=nlp(sent02)\n",
    "doc3=nlp(sent03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4199909d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heres\n",
      "my\n",
      "mail\n",
      "id-\n",
      "vaishxx07@gmail.com\n"
     ]
    }
   ],
   "source": [
    "for token in doc3:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "32de3051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "perusing\n",
      "M.Tech\n",
      "With\n",
      "A.I\n",
      "and\n",
      "D.S\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "307311fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "looking\n",
      "forward\n",
      "to\n",
      "see\n",
      "opportunities\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac84f3fb",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "258efeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a76cd6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_word(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b935266f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'walk walk walk walk'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample='walk walks walking walked '\n",
    "stem_word(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8da127c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'probabl my all time favourit movi is alway be the horror movies,i cannot stop watch them becaus they are my favourit one'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2='Probably my all time favourite movie is always being the horror movies,i cannot stop watching them because they are my favourite ones'\n",
    "stem_word(sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294acee",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b383d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fec9f813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 wa                  \n",
      "running             running             \n",
      "and                 and                 \n",
      "eating              eating              \n",
      "at                  at                  \n",
      "the                 the                 \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 ha                  \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swimming            \n",
      "after               after               \n",
      "playing             playing             \n",
      "long                long                \n",
      "hour                hour                \n",
      "in                  in                  \n",
      "th                  th                  \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "sent='He was running and eating at the same time. He has bad habit of swimming after playing long hour in th Sun.'\n",
    "punct='?:!.,;'\n",
    "sent_word=nltk.word_tokenize(sent)\n",
    "for word in sent_word:\n",
    "    if word in punct:\n",
    "        sent_word.remove(word)\n",
    "\n",
    "sent_word\n",
    "print(\"{0:20}{1:20}\".format('Word','Lemma'))\n",
    "for word in sent_word:\n",
    "    print('{0:20}{1:20}'.format(word,wordnet_lemmatizer.lemmatize(word)))\n",
    "#     print('{0:20}{1:20}'.format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))    #parts of speech given as verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde50df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
